{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to explore predicting victory based on items. The specific champion chosen is not particularly important. I am using Annie because she has ID 1.\n",
    "\n",
    "I am going to try using [Randomized Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html) for this. Ideally we would be able to extract coefficients that tell us which features are important in determining whether Annie wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model as lmod\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import json, glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "CHAMP = 1\n",
    "NUM_ITEMS = 267 # there are 267 items total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preload a mapping from item IDs to indices because the IDs are non-contiguous and have a large range (from the hundreds to 3000+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../datasets/static/item.json', 'r') as f:\n",
    "    items = json.load(f)\n",
    "    item_id_map = {int(k): i for i, k in enumerate(items['data'].keys())}\n",
    "\n",
    "def item_id_to_index(id):\n",
    "    if int(id) in item_id_map:\n",
    "        return item_id_map[id]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the first 10,000 matches one at a time, extract the information we are interested in (items purchased and whether Annie won), and store it in a pair of numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_champ_data():\n",
    "    paths = glob.glob('matches/*.json')[:10000]\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r') as p:\n",
    "            j = json.load(p)\n",
    "            for participant in j['participants']:\n",
    "                stats = participant['stats']\n",
    "                champ = participant['championId']\n",
    "                if not champ == CHAMP:\n",
    "                    continue\n",
    "                items = [stats[\"item0\"],\n",
    "                         stats[\"item1\"],\n",
    "                         stats[\"item2\"],\n",
    "                         stats[\"item3\"],\n",
    "                         stats[\"item4\"],\n",
    "                         stats[\"item5\"],\n",
    "                         stats[\"item6\"]]\n",
    "                winner = stats['winner']\n",
    "\n",
    "                inp = np.zeros((NUM_ITEMS,))\n",
    "                for item in items:\n",
    "                    inp[item_id_to_index(item)] = 1\n",
    "                inputs.append(inp)\n",
    "                outputs.append(winner)\n",
    "    return np.array(inputs), np.array(outputs)\n",
    "\n",
    "inputs, outputs = load_champ_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1371, 267)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1300 data points is a reasonable place to start (I think). My expectations are skewed from vision..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into a training set and a test set. 25% is held out for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_in, test_in, train_out, test_out = train_test_split(inputs, outputs, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedLogisticRegression(C=1, fit_intercept=True,\n",
       "               memory=Memory(cachedir=None), n_jobs=1, n_resampling=200,\n",
       "               normalize=True, pre_dispatch='3*n_jobs', random_state=None,\n",
       "               sample_fraction=0.75, scaling=0.5, selection_threshold=0.25,\n",
       "               tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = lmod.RandomizedLogisticRegression()\n",
    "M.fit(train_in, train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.get_support().nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here our story ends...no features got selected. There are three possibilities: \n",
    "\n",
    "1. The hyperparameter defaults do not work for this data. A grid search could potentially resolve this.\n",
    "2. RLR is not able to select good features for this dataset. I find this unlikely, because I would expect the failure mode to be *too many* features, not too few, in the case of RLR being the 'wrong' tool.\n",
    "3. Items are insufficient to predict win/loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
